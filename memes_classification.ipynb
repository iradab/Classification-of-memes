{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer vision final project\n",
    "## Meme-emotion Classification\n",
    "### Irada Bunyatova & Victor Videau - M2 Artificial Intelligence & Advanced Visual Computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 325,
     "status": "ok",
     "timestamp": 1636122295577,
     "user": {
      "displayName": "Videau Victor",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiNBP6-13AfRga82MvwLmVzsQe3Z3wat4IkTiOFjw=s64",
      "userId": "18131237361494098597"
     },
     "user_tz": -60
    },
    "id": "NWoTfvzcV1mr",
    "outputId": "8ebf10d5-0ea9-45a6-b609-805c5c907371"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "from torchvision import models, transforms\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from PIL import Image\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True # solves OSError: image file is truncated\n",
    "from torch.utils import data\n",
    "import numbers\n",
    "from easynmt import EasyNMT\n",
    "import transformers\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from sklearn.utils import class_weight, shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Sentiment Classification\n",
    "### Given an Internet meme, classify it as a positive, negative or neutral meme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code of this cell taken from https://discuss.pytorch.org/t/how-to-resize-and-pad-in-a-torchvision-transforms-compose/71850/2\n",
    "\n",
    "def get_padding(image):    \n",
    "    w, h = image.size\n",
    "    max_wh = np.max([w, h])\n",
    "    h_padding = (max_wh - w) / 2\n",
    "    v_padding = (max_wh - h) / 2\n",
    "    l_pad = h_padding if h_padding % 1 == 0 else h_padding+0.5\n",
    "    t_pad = v_padding if v_padding % 1 == 0 else v_padding+0.5\n",
    "    r_pad = h_padding if h_padding % 1 == 0 else h_padding-0.5\n",
    "    b_pad = v_padding if v_padding % 1 == 0 else v_padding-0.5\n",
    "    padding = (int(l_pad), int(t_pad), int(r_pad), int(b_pad))\n",
    "    return padding\n",
    "\n",
    "class NewPad(object):\n",
    "    def __init__(self, fill=0, padding_mode='constant'):\n",
    "        assert isinstance(fill, (numbers.Number, str, tuple))\n",
    "        assert padding_mode in ['constant', 'edge', 'reflect', 'symmetric']\n",
    "        self.fill = fill\n",
    "        self.padding_mode = padding_mode\n",
    "        \n",
    "    def __call__(self, img):\n",
    "        return transforms.functional.pad(img, get_padding(img), self.fill, self.padding_mode)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(padding={0}, fill={1}, padding_mode={2})'.\\\n",
    "            format(self.fill, self.padding_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Visualize data augmentation for images </b> <br>\n",
    "Rerun this cell to see different image augmentation examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=224x224 at 0x227C28A0D60>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = Image.open('memotion_dataset_7k/images/image_1.jpg').convert('RGB')\n",
    "transfo = transforms.Compose([\n",
    "    transforms.RandomApply(nn.ModuleList([transforms.RandomCrop(500, pad_if_needed=True)]), p=0.2),\n",
    "    transforms.RandomApply(nn.ModuleList([transforms.ColorJitter(0.2, 0.2, 0.2, 0.03)]), p=0.2),\n",
    "    transforms.RandomHorizontalFlip(p=0.3),\n",
    "    transforms.RandomVerticalFlip(p=0.2),\n",
    "    transforms.RandomPerspective(p=0.2),\n",
    "    NewPad(),\n",
    "    transforms.Resize(224)])\n",
    "\n",
    "transfo(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> load a pre-trained image classifier </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "4skao5xpcNFx"
   },
   "outputs": [],
   "source": [
    "def initialize_model(model_name, num_classes, use_pretrained=True):\n",
    "    \"\"\"function adapted from https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html\n",
    "    \"\"\"\n",
    "    input_size = 224\n",
    "    \n",
    "    if model_name == \"squeezenet\": # Squeezenet : 736 963 parameters\n",
    "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
    "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
    "        model_ft.num_classes = num_classes\n",
    "    \n",
    "    elif model_name == \"densenet\": # Densenet : 6 956 931 parameters\n",
    "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
    "        num_ftrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
    "    \n",
    "    elif model_name == \"resnet\": # Resnet18 : 11 178 051 parameters\n",
    "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "    elif model_name == \"inception\": # Inception v3 : 24 351 718 parameters\n",
    "        model_ft = models.inception_v3(pretrained=use_pretrained)\n",
    "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
    "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 299\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid model name, exiting...\")\n",
    "        exit()\n",
    "\n",
    "    return model_ft, input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prop = 0.8\n",
    "batch_size = 64\n",
    "num_epochs = 5\n",
    "# Models to choose from [squeezenet, densenet, resnet, inception]\n",
    "model_name = 'resnet'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Image Processing and augmentation only for the training images are augmented </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 224 if model_name != 'inception' else 299\n",
    "\n",
    "data_transform_val = transforms.Compose([\n",
    "    NewPad(),\n",
    "    transforms.Resize(input_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.RandomApply(nn.ModuleList([transforms.RandomCrop(500, pad_if_needed=True)]), p=0.3),\n",
    "    transforms.RandomApply(nn.ModuleList([transforms.ColorJitter(0.2, 0.2, 0.2, 0.03)]), p=0.3),\n",
    "    transforms.RandomHorizontalFlip(p=0.4),\n",
    "    transforms.RandomVerticalFlip(p=0.2),\n",
    "    transforms.RandomPerspective(p=0.3),\n",
    "    transforms.RandomAdjustSharpness(50, p=0.2),\n",
    "    NewPad(),\n",
    "    transforms.Resize(input_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 (a) : Sentiment Classification with Images only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=10, is_inception=False):\n",
    "    \"\"\"function adapted from https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html\n",
    "    \"\"\"\n",
    "    since = time.time()\n",
    "    val_acc_history = []\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            \n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    if is_inception and phase == 'train':\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                    preds = torch.max(outputs, 1)[1]\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history\n",
    "\n",
    "\n",
    "class MemeDataSet(data.Dataset):\n",
    "    \n",
    "    def __init__(self, main_dir, transform, ds):\n",
    "        self.img_dir = main_dir+'/images'\n",
    "        self.transform = transform\n",
    "        self.total_imgs = sorted(ds.image_name)\n",
    "        self.labels = ds\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.total_imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_loc = os.path.join(self.img_dir, self.total_imgs[idx])\n",
    "        image = Image.open(img_loc).convert('RGB')\n",
    "        tensor_image = self.transform(image)\n",
    "        image_name = img_loc.replace('\\\\', '/').split('/')[-1]\n",
    "        image_row = self.labels[self.labels.image_name == image_name]\n",
    "        label = image_row.overall_sentiment.item()\n",
    "        return tensor_image, label\n",
    "\n",
    "\n",
    "def process_labels_csv(file_loc, dropna=True):\n",
    "    labels = pd.read_csv(file_loc)\n",
    "    labels.overall_sentiment = labels.overall_sentiment.apply(lambda x: 0 if 'negative' in x else 1 if x == 'neutral' else 2)\n",
    "    labels = labels.drop(['Unnamed: 0', 'text_ocr', 'motivational'], axis=1)\n",
    "    if dropna:\n",
    "        labels = labels.dropna()\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_task1_a, input_size = initialize_model(model_name, num_classes=3, use_pretrained=True)\n",
    "ds = process_labels_csv('memotion_dataset_7k/labels.csv')\n",
    "meme_ids = np.unique(ds.index, return_index=True)[1]\n",
    "train_id, val_id = train_test_split(meme_ids, train_size=train_prop, random_state=69, \\\n",
    "                                    stratify=ds.iloc[meme_ids].overall_sentiment)\n",
    "\n",
    "train_id_img = train_id.copy()  # check to ensure same train-val sets for\n",
    "val_id_img = val_id.copy()      # this model and the upcoming text model\n",
    "ds_train = ds[ds.index.isin(train_id)]\n",
    "ds_val = ds[ds.index.isin(val_id)]\n",
    "meme_dataset_train = MemeDataSet('memotion_dataset_7k', data_transform, ds_train)\n",
    "meme_dataset_val = MemeDataSet('memotion_dataset_7k', data_transform_val, ds_val)\n",
    "train_loader = data.DataLoader(meme_dataset_train, batch_size=batch_size, shuffle=True)\n",
    "val_loader = data.DataLoader(meme_dataset_val, batch_size=batch_size)\n",
    "dataloaders_dict = {'train': train_loader, 'val': val_loader}\n",
    "device = torch.device('cuda:0')\n",
    "model_task1_a, input_size = initialize_model(model_name, num_classes=3, use_pretrained=True)\n",
    "model_task1_a = model_task1_a.to(device)\n",
    "sentiment_labels = ds.overall_sentiment\n",
    "class_w = class_weight.compute_class_weight('balanced', classes=np.unique(sentiment_labels), y=sentiment_labels)\n",
    "criterion = nn.CrossEntropyLoss(weight=torch.Tensor(class_w).to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> If one wants to freeze the first k layers of the pre-trained network </b> <br>\n",
    "Unfortunately it did not help to generalize better on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell from https://discuss.pytorch.org/t/how-the-pytorch-freeze-network-in-some-layers-only-the-rest-of-the-training/7088/2\n",
    "k = 0 # 0 -> no freezing at all\n",
    "for cpt, child in enumerate(model_task1_a.children()):\n",
    "    if cpt < k:\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_to_update = model_task1_a.parameters()\n",
    "optimizer_ft = optim.Adam(params_to_update, lr=0.0001, weight_decay=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/4\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vicvi\\anaconda3\\lib\\site-packages\\PIL\\Image.py:975: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.1835 Acc: 0.3192\n",
      "val Loss: 1.1269 Acc: 0.3264\n",
      "\n",
      "Epoch 1/4\n",
      "----------\n",
      "train Loss: 1.0969 Acc: 0.3907\n",
      "val Loss: 1.1637 Acc: 0.3686\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n",
      "train Loss: 1.0633 Acc: 0.4281\n",
      "val Loss: 1.3083 Acc: 0.4059\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n",
      "train Loss: 1.0062 Acc: 0.4711\n",
      "val Loss: 1.3059 Acc: 0.4896\n",
      "\n",
      "Epoch 4/4\n",
      "----------\n",
      "train Loss: 0.9751 Acc: 0.4831\n",
      "val Loss: 1.3109 Acc: 0.4180\n",
      "\n",
      "Training complete in 10m 43s\n",
      "Best val Acc: 0.489621\n"
     ]
    }
   ],
   "source": [
    "model_task1_a, hist = train_model(model_task1_a, dataloaders_dict, criterion, optimizer_ft, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 (b) : Sentiment Classification with Texts only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemeText(data.Dataset):\n",
    "    \n",
    "    def __init__(self, ds):\n",
    "        texts = list(ds.text_corrected)\n",
    "        tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased', do_lower_case=True)\n",
    "        #tokenizer = AutoTokenizer.from_pretrained('vinai/bertweet-base', do_lower_case=True)\n",
    "        self.encodings = tokenizer(texts, return_tensors='pt', padding='max_length', \\\n",
    "                                   truncation=True, max_length=30) # between 30 and 40\n",
    "        self.labels = np.array(ds.overall_sentiment, dtype=int)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
    "        onehot_lab = torch.Tensor([0, 0, 0])\n",
    "        onehot_lab[self.labels[idx]] = 1\n",
    "        item['labels'] = onehot_lab\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ds = pd.read_csv('memotion_dataset_7k/augmented.csv').drop('Unnamed: 0', axis=1)\n",
    "meme_ids = np.unique(ds.meme_id, return_index=True)[1]\n",
    "train_id, val_id = train_test_split(meme_ids, train_size=train_prop, random_state=69, \\\n",
    "                                    stratify=ds.iloc[meme_ids].overall_sentiment)\n",
    "\n",
    "assert (train_id_img == train_id).mean() == 1, 'Train sets not the same for image and text models'\n",
    "assert (val_id_img == val_id).mean() == 1, 'Validation sets not the same for image and text models'\n",
    "\n",
    "ds_train = ds[ds.meme_id.isin(train_id)]\n",
    "ds_train0 = ds_train[ds_train.overall_sentiment == 0]\n",
    "ds_train1 = ds_train[ds_train.overall_sentiment == 1]\n",
    "ds_train2 = ds_train[ds_train.overall_sentiment == 2]\n",
    "class_sizes = min(np.unique(ds_train.overall_sentiment, return_counts=True)[1])\n",
    "while len(ds_train0) > class_sizes: ds_train0 = ds_train0.drop(np.random.choice(ds_train0.index))\n",
    "while len(ds_train1) > class_sizes: ds_train1 = ds_train1.drop(np.random.choice(ds_train1.index))\n",
    "while len(ds_train2) > class_sizes: ds_train2 = ds_train2.drop(np.random.choice(ds_train2.index))\n",
    "ds_train = pd.concat((ds_train0, ds_train1, ds_train2))\n",
    "ds_val = ds[ds.meme_id.isin(val_id)] # take all text generated\n",
    "ds_val = ds.iloc[val_id] # take only the original sentences\n",
    "train_set = MemeText(ds_train)\n",
    "val_set = MemeText(ds_val)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code of this cell from our NLP course practicals at Ecole Polytechnique\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    labels = np.where(labels == 1)[1].astype(int) # from onehot to positional\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    w_preci = precision_score(labels, preds, average='weighted')\n",
    "    return {'accuracy': accuracy, 'weighted precision': w_preci}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    num_train_epochs=2,              # total number of training epochs\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    learning_rate=1e-4,              # initial learning rate for Adam optimizer\n",
    "    warmup_steps=100,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.02,               # strength of weight decay\n",
    "    output_dir='./results',          # output directory\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=100,               # number of steps to output logging (set lower because of small dataset size)\n",
    "    report_to='all',\n",
    "    evaluation_strategy='steps',     # evaluate during fine-tuning so that we can see progress\n",
    ")\n",
    "\n",
    "def fine_tuning(dataset, test_dataset, compute_metrics, n_lab=3):\n",
    "    model_ft = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=n_lab).to('cuda')\n",
    "    #model_ft = AutoModelForSequenceClassification.from_pretrained('vinai/bertweet-base', num_labels=3).to('cuda')\n",
    "    #config = transformers.DistilBertConfig(dropout=0.5, attention_dropout=0.5, seq_classif_dropout=0.5)\n",
    "    #model_ft = DistilBertForSequenceClassification(config).from_pretrained('distilbert-base-uncased', num_labels=3).to('cuda')\n",
    "    trainer = Trainer(model=model_ft,\n",
    "                      args=training_args,\n",
    "                      train_dataset=dataset,\n",
    "                      eval_dataset=test_dataset,\n",
    "                      compute_metrics=compute_metrics)\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_task1_b = fine_tuning(train_set, val_set, compute_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 19944\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 624\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='624' max='624' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [624/624 00:47, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Weighted precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.640400</td>\n",
       "      <td>0.618253</td>\n",
       "      <td>0.444206</td>\n",
       "      <td>0.467302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.586000</td>\n",
       "      <td>0.657769</td>\n",
       "      <td>0.434907</td>\n",
       "      <td>0.490839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.500100</td>\n",
       "      <td>0.597421</td>\n",
       "      <td>0.502146</td>\n",
       "      <td>0.474512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.348500</td>\n",
       "      <td>0.686783</td>\n",
       "      <td>0.479971</td>\n",
       "      <td>0.483491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.296100</td>\n",
       "      <td>0.720933</td>\n",
       "      <td>0.509299</td>\n",
       "      <td>0.471865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.272700</td>\n",
       "      <td>0.732312</td>\n",
       "      <td>0.501431</td>\n",
       "      <td>0.473098</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1398\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1398\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1398\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1398\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1398\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./results\\checkpoint-500\n",
      "Configuration saved in ./results\\checkpoint-500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1398\n",
      "  Batch size = 64\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=624, training_loss=0.4336520601541568, metrics={'train_runtime': 48.718, 'train_samples_per_second': 818.753, 'train_steps_per_second': 12.808, 'total_flos': 309606669593280.0, 'train_loss': 0.4336520601541568, 'epoch': 2.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_task1_b.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1398\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22' max='22' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [22/22 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.7457491755485535,\n",
       " 'eval_accuracy': 0.47639484978540775,\n",
       " 'eval_weighted precision': 0.46928911825471076,\n",
       " 'eval_runtime': 0.4679,\n",
       " 'eval_samples_per_second': 2987.93,\n",
       " 'eval_steps_per_second': 47.02,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_task1_b.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 (c) : Sentiment Classification - inputs are the Images + Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageTextVoting:\n",
    "    \n",
    "    def __init__(self, image_clf, text_clf):\n",
    "        self.image_clf = image_clf.to('cuda')\n",
    "        self.text_clf = text_clf\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def predict(self, ds_test):\n",
    "        self.image_clf.eval()\n",
    "        self.text_clf.is_in_train = False\n",
    "        image_test_set = MemeDataSet('memotion_dataset_7k', data_transform_val, ds_test)\n",
    "        image_test_loader = data.DataLoader(image_test_set, batch_size=6)\n",
    "        text_test_set = MemeText(ds_test)\n",
    "        text_outputs = self.text_clf.predict(text_test_set)\n",
    "        text_probas = self.softmax(torch.tensor(text_outputs.predictions))\n",
    "        img_outputs = torch.empty((0, 3))\n",
    "        with torch.no_grad():\n",
    "            for img_inputs, img_labels in image_test_loader:\n",
    "                img_inputs = img_inputs.to(device)\n",
    "                img_labels = img_labels.to(device)\n",
    "                batch_output = self.image_clf(img_inputs)\n",
    "                img_outputs = torch.cat((img_outputs, batch_output.to('cpu')))\n",
    "        \n",
    "        img_probas = self.softmax(img_outputs)\n",
    "        probas_vote = 0.5*img_probas + 0.5*text_probas\n",
    "        final_preds = np.argmax(probas_vote, axis=1)\n",
    "        \n",
    "        return final_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/vocab.txt from cache at C:\\Users\\vicvi/.cache\\huggingface\\transformers\\0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer.json from cache at C:\\Users\\vicvi/.cache\\huggingface\\transformers\\75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer_config.json from cache at C:\\Users\\vicvi/.cache\\huggingface\\transformers\\8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at C:\\Users\\vicvi/.cache\\huggingface\\transformers\\23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1398\n",
      "  Batch size = 64\n"
     ]
    }
   ],
   "source": [
    "voting_task1_c = ImageTextVoting(model_task1_a, model_task1_b)\n",
    "y_pred = voting_task1_c.predict(ds_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.5007\n",
      "Weighted precision : 0.4872\n"
     ]
    }
   ],
   "source": [
    "y_true = ds_val.overall_sentiment\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "w_preci = precision_score(y_true, y_pred, average='weighted')\n",
    "print('Accuracy :', round(acc, 4))\n",
    "print('Weighted precision :', round(w_preci, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Humour Classification\n",
    "### Given an Internet meme, identify the type of humor expressed (sarcastic, humorous, and offensive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_labels_csv_humour(file_loc, dropna=True):\n",
    "    labels = pd.read_csv(file_loc)\n",
    "    labels.sarcasm = labels.sarcasm.apply(lambda x: 0 if x == 'not_sarcastic' else 1)\n",
    "    labels.humour = labels.humour.apply(lambda x: 0 if x == 'not_funny' else 1)\n",
    "    labels.offensive = labels.offensive.apply(lambda x: 0 if x == 'not_offensive' else 1)\n",
    "    labels = labels.drop(['Unnamed: 0', 'text_ocr', 'motivational'], axis=1)\n",
    "    if dropna:\n",
    "        labels = labels.dropna()\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 (a) : Humour Classification with Images only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_humour(model, dataloaders, criterion, optimizer, num_epochs=10, is_inception=False):\n",
    "    since = time.time()\n",
    "    val_acc_history = []\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            \n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    if is_inception and phase == 'train':\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                        tens = torch.zeros(len(outputs), 3)\n",
    "                        for i in range(0,len(outputs)):\n",
    "                            for j in range(0,3):\n",
    "                                if (outputs[i,j].item() >= 0 and labels[i,j] == 1):\n",
    "                                    tens[i,j] = 1\n",
    "                                elif(outputs[i,j].item() < 0 and labels[i,j] == 0):\n",
    "                                    tens[i,j] = 1\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += tens.sum()\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            \n",
    "            epoch_acc = running_corrects.double() / (len(dataloaders[phase].dataset) * 3) # because we have three classes\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history\n",
    "\n",
    "\n",
    "class MemeDataSetHumour(data.Dataset):\n",
    "    \n",
    "    def __init__(self, main_dir, transform, ds):\n",
    "        self.img_dir = main_dir+'/images'\n",
    "        self.transform = transform\n",
    "        self.total_imgs = sorted(ds.image_name)\n",
    "        self.labels = ds\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.total_imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_loc = os.path.join(self.img_dir, self.total_imgs[idx])\n",
    "        image = Image.open(img_loc).convert('RGB')\n",
    "        tensor_image = self.transform(image)\n",
    "        image_name = img_loc.replace('\\\\', '/').split('/')[-1]\n",
    "        image_row = self.labels[self.labels.image_name == image_name]\n",
    "        label1 = image_row.sarcasm.item()\n",
    "        label2 = image_row.humour.item()\n",
    "        label3 = image_row.offensive.item()\n",
    "        label = torch.Tensor([label1, label2, label3])\n",
    "        return tensor_image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = process_labels_csv_humour('memotion_dataset_7k/labels.csv')\n",
    "ds.overall_sentiment = ds.overall_sentiment.apply(lambda x: 0 if 'negative' in x else 1 if x == 'neutral' else 2)\n",
    "meme_ids = np.unique(ds.index, return_index=True)[1]\n",
    "train_id, val_id = train_test_split(meme_ids, train_size=train_prop, random_state=69)\n",
    "train_id_img = train_id.copy()  # check to ensure same train-val sets for\n",
    "val_id_img = val_id.copy()      # this model and the upcoming text model\n",
    "\n",
    "ds_train = ds[ds.index.isin(train_id)]\n",
    "ds_val = ds[ds.index.isin(val_id)]\n",
    "meme_dataset_train = MemeDataSetHumour('memotion_dataset_7k', data_transform, ds_train)\n",
    "meme_dataset_val = MemeDataSetHumour('memotion_dataset_7k', data_transform_val, ds_val)\n",
    "train_loader = data.DataLoader(meme_dataset_train, batch_size=batch_size, shuffle=True)\n",
    "val_loader = data.DataLoader(meme_dataset_val, batch_size=batch_size)\n",
    "dataloaders_dict = {'train': train_loader, 'val': val_loader}\n",
    "device = torch.device('cuda:0')\n",
    "model_task2_a, input_size = initialize_model(model_name, num_classes=3, use_pretrained=True)\n",
    "model_task2_a = model_task2_a.to(device)\n",
    "\n",
    "meme_dataset_humour = ds\n",
    "bin_humour = np.bincount(meme_dataset_humour.humour)\n",
    "w_humour = bin_humour[0] / bin_humour[1]\n",
    "bin_sarcasm = np.bincount(meme_dataset_humour.sarcasm)\n",
    "w_sarcasm = bin_sarcasm[0] / bin_sarcasm[1]\n",
    "bin_off = np.bincount(meme_dataset_humour.offensive)\n",
    "w_off = bin_off[0] / bin_off[1]\n",
    "bin_class_w = torch.Tensor([w_sarcasm, w_humour, w_off]).to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=bin_class_w)\n",
    "params_to_update = model_task2_a.parameters()\n",
    "optimizer_ft = optim.Adam(params_to_update, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/4\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vicvi\\anaconda3\\lib\\site-packages\\PIL\\Image.py:975: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.4043 Acc: 0.5143\n",
      "val Loss: 0.4006 Acc: 0.4831\n",
      "\n",
      "Epoch 1/4\n",
      "----------\n",
      "train Loss: 0.3874 Acc: 0.5528\n",
      "val Loss: 0.4016 Acc: 0.5043\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n",
      "train Loss: 0.3796 Acc: 0.5722\n",
      "val Loss: 0.4037 Acc: 0.5286\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n",
      "train Loss: 0.3704 Acc: 0.6040\n",
      "val Loss: 0.4154 Acc: 0.4683\n",
      "\n",
      "Epoch 4/4\n",
      "----------\n",
      "train Loss: 0.3585 Acc: 0.6292\n",
      "val Loss: 0.4259 Acc: 0.4814\n",
      "\n",
      "Training complete in 14m 14s\n",
      "Best val Acc: 0.528612\n"
     ]
    }
   ],
   "source": [
    "model_task2_a, hist = train_model_humour(model_task2_a, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 (b) : Humour Classification with Texts only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemeTextHumour(data.Dataset):\n",
    "    \n",
    "    def __init__(self, ds):\n",
    "        texts = list(ds.text_corrected)\n",
    "        tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased', do_lower_case=True)\n",
    "        #tokenizer = AutoTokenizer.from_pretrained('vinai/bertweet-base', do_lower_case=True)\n",
    "        self.encodings = tokenizer(texts, return_tensors='pt', padding='max_length', \\\n",
    "                                   truncation=True, max_length=30) # between 30 and 40\n",
    "        self.humour = np.array(ds.humour, dtype=int)\n",
    "        self.sarcasm = np.array(ds.sarcasm, dtype=int)\n",
    "        self.offensive = np.array(ds.offensive, dtype=int)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.humour)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
    "        onehot_lab = torch.Tensor([0, 0, 0])\n",
    "        onehot_lab[0] = self.sarcasm[idx]\n",
    "        onehot_lab[1] = self.humour[idx]\n",
    "        onehot_lab[2] = self.offensive[idx]\n",
    "        item['labels'] = onehot_lab\n",
    "        return item\n",
    "\n",
    "\n",
    "def compute_metrics_humour(pred):\n",
    "    labels = pred.label_ids\n",
    "    outputs = pred.predictions\n",
    "    tens = torch.zeros(labels.shape[0], labels.shape[1])\n",
    "    for i in range(len(outputs)):\n",
    "        for j in range(labels.shape[1]):\n",
    "            if (outputs[i,j].item() >= 0 and labels[i,j] == 1):\n",
    "                tens[i,j] = 1\n",
    "            elif(outputs[i,j].item() < 0 and labels[i,j] == 0):\n",
    "                tens[i,j] = 1\n",
    "    accuracy = tens.mean()\n",
    "    return {\n",
    "      'accuracy': accuracy,\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/vocab.txt from cache at C:\\Users\\vicvi/.cache\\huggingface\\transformers\\0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer.json from cache at C:\\Users\\vicvi/.cache\\huggingface\\transformers\\75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer_config.json from cache at C:\\Users\\vicvi/.cache\\huggingface\\transformers\\8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at C:\\Users\\vicvi/.cache\\huggingface\\transformers\\23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/vocab.txt from cache at C:\\Users\\vicvi/.cache\\huggingface\\transformers\\0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer.json from cache at C:\\Users\\vicvi/.cache\\huggingface\\transformers\\75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer_config.json from cache at C:\\Users\\vicvi/.cache\\huggingface\\transformers\\8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at C:\\Users\\vicvi/.cache\\huggingface\\transformers\\23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ds = pd.read_csv('memotion_dataset_7k/augmented.csv').drop('Unnamed: 0', axis=1)\n",
    "ds.sarcasm = ds.sarcasm.apply(lambda x: 0 if x == 'not_sarcastic' else 1)\n",
    "ds.humour = ds.humour.apply(lambda x: 0 if x == 'not_funny' else 1)\n",
    "ds.offensive = ds.offensive.apply(lambda x: 0 if x == 'not_offensive' else 1)\n",
    "\n",
    "meme_ids = np.unique(ds.meme_id, return_index=True)[1]\n",
    "train_id, val_id = train_test_split(meme_ids, train_size=train_prop, random_state=69)\n",
    "\n",
    "assert (train_id_img == train_id).mean() == 1, 'Train sets not the same for image and text models'\n",
    "assert (val_id_img == val_id).mean() == 1, 'Validation sets not the same for image and text models'\n",
    "\n",
    "ds_train = ds[ds.meme_id.isin(train_id)]\n",
    "ds_sarcasm = ds_train[ds_train.sarcasm == 0]\n",
    "ds_sarcasm_1 = ds_train[ds_train.sarcasm == 1]\n",
    "ds_balanced = pd.concat((ds_sarcasm, shuffle(ds_sarcasm_1, random_state=69).iloc[:4582]))\n",
    "ds_train = ds_balanced\n",
    "\n",
    "ds_val = ds[ds.meme_id.isin(val_id)] # take all text generated\n",
    "ds_val = ds.iloc[val_id] # take only the original sentences\n",
    "train_set = MemeTextHumour(ds_train)\n",
    "val_set = MemeTextHumour(ds_val)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "using `logging_steps` to initialize `eval_steps` to 80\n",
      "PyTorch: setting up devices\n",
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at C:\\Users\\vicvi/.cache\\huggingface\\transformers\\23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at C:\\Users\\vicvi/.cache\\huggingface\\transformers\\9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    learning_rate=1e-4,              # initial learning rate for Adam optimizer\n",
    "    warmup_steps=100,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.02,               # strength of weight decay\n",
    "    output_dir='./results',          # output directory\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=80,               # number of steps to output logging (set lower because of small dataset size)\n",
    "    report_to='all',\n",
    "    evaluation_strategy='steps',     # evaluate during fine-tuning so that we can see progress\n",
    ")\n",
    "\n",
    "model_task2_b = fine_tuning(train_set, val_set, compute_metrics_humour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 9402\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 441\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='441' max='441' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [441/441 00:53, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.681400</td>\n",
       "      <td>0.652714</td>\n",
       "      <td>0.600620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.640500</td>\n",
       "      <td>0.709854</td>\n",
       "      <td>0.615641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.511700</td>\n",
       "      <td>0.716523</td>\n",
       "      <td>0.632093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.406100</td>\n",
       "      <td>0.871705</td>\n",
       "      <td>0.608727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.263500</td>\n",
       "      <td>0.971327</td>\n",
       "      <td>0.597997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1398\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1398\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1398\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1398\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1398\n",
      "  Batch size = 64\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=441, training_loss=0.47676870125491605, metrics={'train_runtime': 53.7273, 'train_samples_per_second': 524.985, 'train_steps_per_second': 8.208, 'total_flos': 218932153092360.0, 'train_loss': 0.47676870125491605, 'epoch': 3.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_task2_b.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1398\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='66' max='22' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [22/22 46:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.9673082232475281,\n",
       " 'eval_accuracy': 0.5968049764633179,\n",
       " 'eval_runtime': 0.7445,\n",
       " 'eval_samples_per_second': 1877.793,\n",
       " 'eval_steps_per_second': 29.55,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_task2_b.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 (c) : Humour Classification with Images and Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageTextVotingTask2:\n",
    "    \n",
    "    def __init__(self, image_clf, text_clf):\n",
    "        self.image_clf = image_clf.to('cuda')\n",
    "        self.text_clf = text_clf\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def predict(self, ds_test):\n",
    "        self.image_clf.eval()\n",
    "        self.text_clf.is_in_train = False\n",
    "        image_test_set = MemeDataSetHumour('memotion_dataset_7k', data_transform_val, ds_test)\n",
    "        image_test_loader = data.DataLoader(image_test_set, batch_size=6)\n",
    "        text_test_set = MemeTextHumour(ds_test)\n",
    "        text_outputs = self.text_clf.predict(text_test_set)\n",
    "        sigm = nn.Sigmoid()\n",
    "        text_probas = sigm(torch.tensor(text_outputs.predictions))\n",
    "        img_outputs = torch.empty((0, 3))\n",
    "        with torch.no_grad():\n",
    "            for img_inputs, img_labels in image_test_loader:\n",
    "                img_inputs = img_inputs.to(device)\n",
    "                img_labels = img_labels.to(device)\n",
    "                batch_output = self.image_clf(img_inputs).to('cpu')\n",
    "                img_outputs = torch.cat((img_outputs, sigm(batch_output)))\n",
    "                \n",
    "        img_probas = img_outputs\n",
    "        probas_vote = 0.5*img_probas + 0.5*text_probas\n",
    "        final_preds = (probas_vote >= 0.5).numpy().astype(int)\n",
    "        \n",
    "        return final_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/vocab.txt from cache at C:\\Users\\vicvi/.cache\\huggingface\\transformers\\0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer.json from cache at C:\\Users\\vicvi/.cache\\huggingface\\transformers\\75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer_config.json from cache at C:\\Users\\vicvi/.cache\\huggingface\\transformers\\8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at C:\\Users\\vicvi/.cache\\huggingface\\transformers\\23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1398\n",
      "  Batch size = 64\n",
      "C:\\Users\\vicvi\\anaconda3\\lib\\site-packages\\PIL\\Image.py:975: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "voting_task2 = ImageTextVotingTask2(model_task2_a, model_task2_b)\n",
    "y_pred = voting_task2.predict(ds_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t    Accuracy\n",
      "sarcasm      0.6016\n",
      "humour       0.6924\n",
      "offensive    0.5236\n",
      "dtype: float64\n",
      "\n",
      "mean accuracy : 0.6059\n"
     ]
    }
   ],
   "source": [
    "y_true = ds_val[['sarcasm', 'humour', 'offensive']]\n",
    "acc = (y_true == y_pred).mean()\n",
    "print('\\t\\t    Accuracy\\n', round(acc, 4), sep='')\n",
    "print('\\nmean accuracy :', round(acc.mean(), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Multi-task classification\n",
    "### Create one model that solves both Tasks 1 and 2 at the same time<br>\n",
    "## Task 3 (a) : Multi-task classification with Images only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_task3(model, dataloaders, criterion, optimizer, num_epochs=10, is_inception=False):\n",
    "    since = time.time()\n",
    "    val_acc_history = []\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            \n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    if is_inception and phase == 'train':\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                        acc = 0\n",
    "                        for i in range(0,len(outputs)):\n",
    "                            for j in range(0,3):\n",
    "                                if (outputs[i,j].item() >= 0 and labels[i,j] == 1):\n",
    "                                    acc += 0.25\n",
    "                                elif(outputs[i,j].item() < 0 and labels[i,j] == 0):\n",
    "                                    acc += 0.25\n",
    "                            if labels[i, 3:] [np.argmax(outputs[i, 3:].detach().cpu())] == 1:\n",
    "                                acc += 0.25\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += acc\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            \n",
    "            epoch_acc = running_corrects / (len(dataloaders[phase].dataset))\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history\n",
    "\n",
    "\n",
    "class MemeDataSetTask3(data.Dataset):\n",
    "    \n",
    "    def __init__(self, main_dir, transform, ds):\n",
    "        self.img_dir = main_dir+'/images'\n",
    "        self.transform = transform\n",
    "        self.total_imgs = sorted(ds.image_name)\n",
    "        self.labels = ds\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.total_imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_loc = os.path.join(self.img_dir, self.total_imgs[idx])\n",
    "        image = Image.open(img_loc).convert('RGB')\n",
    "        tensor_image = self.transform(image)\n",
    "        image_name = img_loc.replace('\\\\', '/').split('/')[-1]\n",
    "        image_row = self.labels[self.labels.image_name == image_name]\n",
    "        label1 = image_row.sarcasm.item()\n",
    "        label2 = image_row.humour.item()\n",
    "        label3 = image_row.offensive.item()\n",
    "        label4 = image_row.overall_sentiment.item()\n",
    "        label = torch.zeros(6)\n",
    "        label[0] = label1\n",
    "        label[1] = label2\n",
    "        label[2] = label3\n",
    "        label[3+label4] = 1\n",
    "        \n",
    "        return tensor_image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ds = process_labels_csv_humour('memotion_dataset_7k/labels.csv')\n",
    "ds.overall_sentiment = ds.overall_sentiment.apply(lambda x: 0 if 'negative' in x else 1 if x == 'neutral' else 2)\n",
    "meme_ids = np.unique(ds.index, return_index=True)[1]\n",
    "train_id, val_id = train_test_split(meme_ids, train_size=train_prop, random_state=69)\n",
    "train_id_img = train_id.copy()  # check to ensure same train-val sets for\n",
    "val_id_img = val_id.copy()      # this model and the upcoming text model\n",
    "\n",
    "ds_train = ds[ds.index.isin(train_id)]\n",
    "ds_val = ds[ds.index.isin(val_id)]\n",
    "meme_dataset_train = MemeDataSetTask3('memotion_dataset_7k', data_transform, ds_train)\n",
    "meme_dataset_val = MemeDataSetTask3('memotion_dataset_7k', data_transform_val, ds_val)\n",
    "train_loader = data.DataLoader(meme_dataset_train, batch_size=batch_size, shuffle=True)\n",
    "val_loader = data.DataLoader(meme_dataset_val, batch_size=batch_size)\n",
    "dataloaders_dict = {'train': train_loader, 'val': val_loader}\n",
    "device = torch.device('cuda:0')\n",
    "model_task3_a, input_size = initialize_model(model_name, num_classes=6, use_pretrained=True)\n",
    "model_task3_a = model_task3_a.to(device)\n",
    "\n",
    "bin_humour = np.bincount(ds.humour)\n",
    "w_humour = bin_humour[0] / bin_humour[1]\n",
    "bin_sarcasm = np.bincount(ds.sarcasm)\n",
    "w_sarcasm = bin_sarcasm[0] / bin_sarcasm[1]\n",
    "bin_off = np.bincount(ds.offensive)\n",
    "w_off = bin_off[0] / bin_off[1]\n",
    "w_class4 = (len(ds_train) - len(ds_train[ds_train['overall_sentiment']==0])) / len(ds_train['overall_sentiment']==0)\n",
    "w_class5 = (len(ds_train) - len(ds_train[ds_train['overall_sentiment']==1])) / len(ds_train['overall_sentiment']==1)\n",
    "w_class6 = (len(ds_train) - len(ds_train[ds_train['overall_sentiment']==2])) / len(ds_train['overall_sentiment']==2)\n",
    "bin_class_w = torch.Tensor([w_sarcasm, w_humour, w_off, w_class4, w_class5, w_class6]).to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=bin_class_w)\n",
    "params_to_update = model_task3_a.parameters()\n",
    "optimizer_ft = optim.Adam(params_to_update, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/4\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vicvi\\anaconda3\\lib\\site-packages\\PIL\\Image.py:975: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.4382 Acc: 0.4942\n",
      "val Loss: 0.4099 Acc: 0.5536\n",
      "\n",
      "Epoch 1/4\n",
      "----------\n",
      "train Loss: 0.3934 Acc: 0.5514\n",
      "val Loss: 0.4107 Acc: 0.5438\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n",
      "train Loss: 0.3859 Acc: 0.5793\n",
      "val Loss: 0.4149 Acc: 0.4928\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n",
      "train Loss: 0.3766 Acc: 0.6001\n",
      "val Loss: 0.4223 Acc: 0.5177\n",
      "\n",
      "Epoch 4/4\n",
      "----------\n",
      "train Loss: 0.3710 Acc: 0.6076\n",
      "val Loss: 0.4250 Acc: 0.5020\n",
      "\n",
      "Training complete in 10m 31s\n",
      "Best val Acc: 0.553648\n"
     ]
    }
   ],
   "source": [
    "model_task3_a, hist = train_model_task3(model_task3_a, dataloaders_dict, criterion, optimizer_ft, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 (b) : Multi-task classification with Texts only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemeTextTask3(data.Dataset):\n",
    "    \n",
    "    def __init__(self, ds):\n",
    "        texts = list(ds.text_corrected)\n",
    "        tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased', do_lower_case=True)\n",
    "        #tokenizer = AutoTokenizer.from_pretrained('vinai/bertweet-base', do_lower_case=True)\n",
    "        self.encodings = tokenizer(texts, return_tensors='pt', padding='max_length', \\\n",
    "                                   truncation=True, max_length=30) # between 30 and 40\n",
    "        self.humour = np.array(ds.humour, dtype=int)\n",
    "        self.sarcasm = np.array(ds.sarcasm, dtype=int)\n",
    "        self.offensive = np.array(ds.offensive, dtype=int)\n",
    "        self.overall_sentiment = np.array(ds.overall_sentiment, dtype=int)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.humour)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
    "        onehot_lab = torch.zeros(6)\n",
    "        onehot_lab[0] = self.sarcasm[idx]\n",
    "        onehot_lab[1] = self.humour[idx]\n",
    "        onehot_lab[2] = self.offensive[idx]\n",
    "        onehot_lab[3+self.overall_sentiment[idx]] = 1\n",
    "        item['labels'] = onehot_lab\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/vocab.txt from cache at C:\\Users\\vicvi/.cache\\huggingface\\transformers\\0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer.json from cache at C:\\Users\\vicvi/.cache\\huggingface\\transformers\\75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer_config.json from cache at C:\\Users\\vicvi/.cache\\huggingface\\transformers\\8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at C:\\Users\\vicvi/.cache\\huggingface\\transformers\\23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/vocab.txt from cache at C:\\Users\\vicvi/.cache\\huggingface\\transformers\\0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer.json from cache at C:\\Users\\vicvi/.cache\\huggingface\\transformers\\75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer_config.json from cache at C:\\Users\\vicvi/.cache\\huggingface\\transformers\\8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at C:\\Users\\vicvi/.cache\\huggingface\\transformers\\23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "using `logging_steps` to initialize `eval_steps` to 50\n",
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "ds = pd.read_csv('memotion_dataset_7k/augmented.csv').drop('Unnamed: 0', axis=1)\n",
    "ds.sarcasm = ds.sarcasm.apply(lambda x: 0 if x == 'not_sarcastic' else 1)\n",
    "ds.humour = ds.humour.apply(lambda x: 0 if x == 'not_funny' else 1)\n",
    "ds.offensive = ds.offensive.apply(lambda x: 0 if x == 'not_offensive' else 1)\n",
    "\n",
    "meme_ids = np.unique(ds.meme_id, return_index=True)[1]\n",
    "train_id, val_id = train_test_split(meme_ids, train_size=train_prop, random_state=69)\n",
    "\n",
    "assert (train_id_img == train_id).mean() == 1, 'Train sets not the same for image and text models'\n",
    "assert (val_id_img == val_id).mean() == 1, 'Validation sets not the same for image and text models'\n",
    "\n",
    "ds_train = ds[ds.meme_id.isin(train_id)]\n",
    "ds_sarcasm = ds_train[ds_train.sarcasm == 0]\n",
    "ds_sarcasm_1 = ds_train[ds_train.sarcasm == 1]\n",
    "ds_balanced = pd.concat((ds_sarcasm, shuffle(ds_sarcasm_1, random_state=69).iloc[:4582]))\n",
    "ds_train = ds_balanced\n",
    "\n",
    "# ds_val = ds[ds.meme_id.isin(val_id)] # take all text generated\n",
    "ds_val = ds.iloc[val_id] # take only the original sentences\n",
    "train_set = MemeTextTask3(ds_train)\n",
    "val_set = MemeTextTask3(ds_val)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    num_train_epochs=2,              # total number of training epochs\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    learning_rate=1e-4,              # initial learning rate for Adam optimizer\n",
    "    warmup_steps=100,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.02,               # strength of weight decay\n",
    "    output_dir='./results',          # output directory\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=50,               # number of steps to output logging (set lower because of small dataset size)\n",
    "    report_to='all',\n",
    "    evaluation_strategy='steps',     # evaluate during fine-tuning so that we can see progress\n",
    ")\n",
    "\n",
    "def compute_metrics_task3(pred):\n",
    "    global labels\n",
    "    labels = pred.label_ids\n",
    "    outputs = pred.predictions\n",
    "    acc = 0\n",
    "    for i in range(len(outputs)):\n",
    "        for j in range(3):\n",
    "            if (outputs[i,j].item() >= 0 and labels[i,j] == 1):\n",
    "                acc += 0.25\n",
    "            elif(outputs[i,j].item() < 0 and labels[i,j] == 0):\n",
    "                acc += 0.25\n",
    "        if labels[i, 3:] [np.argmax(outputs[i, 3:])] == 1:\n",
    "            acc += 0.25\n",
    "    acc /= labels.shape[0]\n",
    "    return {\n",
    "      'accuracy': torch.tensor([acc]),\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at C:\\Users\\vicvi/.cache\\huggingface\\transformers\\23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at C:\\Users\\vicvi/.cache\\huggingface\\transformers\\9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_task3_b = fine_tuning(train_set, val_set, compute_metrics_task3, n_lab=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 9402\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 294\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='294' max='294' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [294/294 00:22, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.670900</td>\n",
       "      <td>0.651323</td>\n",
       "      <td>0.594599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.655100</td>\n",
       "      <td>0.643449</td>\n",
       "      <td>0.483906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.623500</td>\n",
       "      <td>0.631836</td>\n",
       "      <td>0.518240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.545600</td>\n",
       "      <td>0.667308</td>\n",
       "      <td>0.556509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.495500</td>\n",
       "      <td>0.697617</td>\n",
       "      <td>0.533798</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1398\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1398\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1398\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1398\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1398\n",
      "  Batch size = 64\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=294, training_loss=0.5799547896093252, metrics={'train_runtime': 23.11, 'train_samples_per_second': 813.674, 'train_steps_per_second': 12.722, 'total_flos': 145962577277280.0, 'train_loss': 0.5799547896093252, 'epoch': 2.0})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_task3_b.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1398\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='44' max='22' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [22/22 00:18]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6669305562973022,\n",
       " 'eval_accuracy': 0.5729613900184631,\n",
       " 'eval_runtime': 0.4856,\n",
       " 'eval_samples_per_second': 2878.821,\n",
       " 'eval_steps_per_second': 45.303,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_task3_b.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 (c) : Multi-task classification with Images and Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageTextVotingTask3:\n",
    "    \n",
    "    def __init__(self, image_clf, text_clf):\n",
    "        self.image_clf = image_clf.to('cuda')\n",
    "        self.text_clf = text_clf\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def predict(self, ds_test):\n",
    "        self.image_clf.eval()\n",
    "        self.text_clf.is_in_train = False\n",
    "        image_test_set = MemeDataSetTask3('memotion_dataset_7k', data_transform_val, ds_test)\n",
    "        image_test_loader = data.DataLoader(image_test_set, batch_size=6)\n",
    "        text_test_set = MemeTextTask3(ds_test)\n",
    "        text_outputs = self.text_clf.predict(text_test_set)\n",
    "        sigm = nn.Sigmoid()\n",
    "        text_probas = sigm(torch.tensor(text_outputs.predictions))\n",
    "        img_outputs = torch.empty((0, 6))\n",
    "        with torch.no_grad():\n",
    "            for img_inputs, img_labels in image_test_loader:\n",
    "                img_inputs = img_inputs.to(device)\n",
    "                img_labels = img_labels.to(device)\n",
    "                batch_output = self.image_clf(img_inputs).to('cpu')\n",
    "                img_outputs = torch.cat((img_outputs, sigm(batch_output)))\n",
    "                \n",
    "        img_probas = img_outputs\n",
    "        probas_vote = 0.5*img_probas + 0.5*text_probas\n",
    "        \n",
    "        final_preds = np.zeros((probas_vote.shape[0], 4))\n",
    "        final_preds[:, :3] = (probas_vote[:, :3] >= 0.5).numpy().astype(int)\n",
    "        final_preds[:, 3] = np.argmax(probas_vote[:, 3:], axis=1)\n",
    "        \n",
    "        return final_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/vocab.txt from cache at C:\\Users\\vicvi/.cache\\huggingface\\transformers\\0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer.json from cache at C:\\Users\\vicvi/.cache\\huggingface\\transformers\\75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer_config.json from cache at C:\\Users\\vicvi/.cache\\huggingface\\transformers\\8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at C:\\Users\\vicvi/.cache\\huggingface\\transformers\\23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1398\n",
      "  Batch size = 64\n"
     ]
    }
   ],
   "source": [
    "voting_task3 = ImageTextVotingTask3(model_task3_a, model_task3_b)\n",
    "y_pred = voting_task3.predict(ds_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6295000000000001"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.6338+0.7225+0.5322)/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t    Accuracy\n",
      "sarcasm              0.6338\n",
      "humour               0.7225\n",
      "offensive            0.5322\n",
      "overall_sentiment    0.4735\n",
      "dtype: float64\n",
      "\n",
      "mean accuracy : 0.5905\n"
     ]
    }
   ],
   "source": [
    "y_true = ds_val[['sarcasm', 'humour', 'offensive', 'overall_sentiment']]\n",
    "acc = (y_true == y_pred).mean()\n",
    "print('\\t\\t    Accuracy\\n', round(acc, 4), sep='')\n",
    "print('\\nmean accuracy :', round(acc.mean(), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus : how the Data Augmentation was done for texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/Helsinki-NLP/opus-mt-en-fr/resolve/main/source.spm from cache at C:\\Users\\vicvi/.cache\\huggingface\\transformers\\eca4c919c3d3cc5faaa381373f5a5fe2c0eeaa29d31b42d55268045437669a82.1a8b1c99c8359ed99f2d577f69114f5e285203705b08e5b9177f626b259660ec\n",
      "loading file https://huggingface.co/Helsinki-NLP/opus-mt-en-fr/resolve/main/target.spm from cache at C:\\Users\\vicvi/.cache\\huggingface\\transformers\\a2d9a090633037aae001b187247ddb1bd8ba642781d94be946f8b5135fafb32d.7a3fadd05a0cee82a22786164d20d49e7b313753bf53c7e219cd382f47c08871\n",
      "loading file https://huggingface.co/Helsinki-NLP/opus-mt-en-fr/resolve/main/vocab.json from cache at C:\\Users\\vicvi/.cache\\huggingface\\transformers\\d423eff1e327872c07447b1985804b7359a78d2491c3a17a26e392ce9f7cda06.e226d68e114993b873bdf547cbc62c903a4d4f7bfbe15ff8ba40ed3136bcacec\n",
      "loading file https://huggingface.co/Helsinki-NLP/opus-mt-en-fr/resolve/main/tokenizer_config.json from cache at C:\\Users\\vicvi/.cache\\huggingface\\transformers\\0b4ef0241eeb4fd99663d26760c8c2df94c3e2f94d22a903e92ef3cafbb2889a.509446054fb7fa5c958c78bb55c57f18152eb3fee9418b8c53626e2d266628bd\n",
      "loading file https://huggingface.co/Helsinki-NLP/opus-mt-en-fr/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/Helsinki-NLP/opus-mt-en-fr/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/Helsinki-NLP/opus-mt-en-fr/resolve/main/tokenizer.json from cache at None\n",
      "loading configuration file https://huggingface.co/Helsinki-NLP/opus-mt-en-fr/resolve/main/config.json from cache at C:\\Users\\vicvi/.cache\\huggingface\\transformers\\5ad88432037ab18b1eb95761258d2b1b3a32e1e401d5f610f86eb3f479e59e8c.2b4f07b3f8de3922d42e6312c55d0597e44d2273507e7c5d0b6daf75fb2cc673\n",
      "Model config MarianConfig {\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      59513\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 59513,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 59513,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 59514\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/Helsinki-NLP/opus-mt-en-fr/resolve/main/config.json from cache at C:\\Users\\vicvi/.cache\\huggingface\\transformers\\5ad88432037ab18b1eb95761258d2b1b3a32e1e401d5f610f86eb3f479e59e8c.2b4f07b3f8de3922d42e6312c55d0597e44d2273507e7c5d0b6daf75fb2cc673\n",
      "Model config MarianConfig {\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      59513\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 59513,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 59513,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 59514\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/Helsinki-NLP/opus-mt-en-fr/resolve/main/pytorch_model.bin from cache at C:\\Users\\vicvi/.cache\\huggingface\\transformers\\df7925366c58e84bcbddeab1100929d66148fffbba2fdacddfa3bc9b396c5dc8.6e73740b296c2a7675d00232d5974e8056cf443063edf436f82939e35e9eef29\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at Helsinki-NLP/opus-mt-en-fr.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading file https://huggingface.co/Helsinki-NLP/opus-mt-fr-en/resolve/main/source.spm from cache at C:\\Users\\vicvi/.cache\\huggingface\\transformers\\219a0cde9ac0e8341de27f673e2bb190e61fcfce0abbba164d3236ad7a158502.7a3fadd05a0cee82a22786164d20d49e7b313753bf53c7e219cd382f47c08871\n",
      "loading file https://huggingface.co/Helsinki-NLP/opus-mt-fr-en/resolve/main/target.spm from cache at C:\\Users\\vicvi/.cache\\huggingface\\transformers\\20a1dcb26507d373a88c44d04c85bb9d50b185955846ae8db5550ed1582c8e57.1a8b1c99c8359ed99f2d577f69114f5e285203705b08e5b9177f626b259660ec\n",
      "loading file https://huggingface.co/Helsinki-NLP/opus-mt-fr-en/resolve/main/vocab.json from cache at C:\\Users\\vicvi/.cache\\huggingface\\transformers\\7714e23dcbc0a871909fb2fc4b869614c455c25a7b6336a53bbac3788cd129ce.e226d68e114993b873bdf547cbc62c903a4d4f7bfbe15ff8ba40ed3136bcacec\n",
      "loading file https://huggingface.co/Helsinki-NLP/opus-mt-fr-en/resolve/main/tokenizer_config.json from cache at C:\\Users\\vicvi/.cache\\huggingface\\transformers\\7c27fc1835084e1c8b737a17bba880cd3222f2ecfd1b0c8094d4bab5b980ca82.6d8b2abecd7660f5771888eba1b0c54ea352310e2f18ff8178fdb14a9b1bf4e0\n",
      "loading file https://huggingface.co/Helsinki-NLP/opus-mt-fr-en/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/Helsinki-NLP/opus-mt-fr-en/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/Helsinki-NLP/opus-mt-fr-en/resolve/main/tokenizer.json from cache at None\n",
      "loading configuration file https://huggingface.co/Helsinki-NLP/opus-mt-fr-en/resolve/main/config.json from cache at C:\\Users\\vicvi/.cache\\huggingface\\transformers\\4245a4a07545aad84ab5538aeb19472410cea9ae5a0d63ab2973f3fcfc66ac39.2b4f07b3f8de3922d42e6312c55d0597e44d2273507e7c5d0b6daf75fb2cc673\n",
      "Model config MarianConfig {\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      59513\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 59513,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 59513,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 59514\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/Helsinki-NLP/opus-mt-fr-en/resolve/main/config.json from cache at C:\\Users\\vicvi/.cache\\huggingface\\transformers\\4245a4a07545aad84ab5538aeb19472410cea9ae5a0d63ab2973f3fcfc66ac39.2b4f07b3f8de3922d42e6312c55d0597e44d2273507e7c5d0b6daf75fb2cc673\n",
      "Model config MarianConfig {\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      59513\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 59513,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 59513,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 59514\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/Helsinki-NLP/opus-mt-fr-en/resolve/main/pytorch_model.bin from cache at C:\\Users\\vicvi/.cache\\huggingface\\transformers\\36cdc34a2d8d567071088aeb3eb9949af7c4cffd090095fa7ab5e44d9130d47e.c37cbeed2c35adab9ae11f696e037a81cd4152101eb7256952cee91482686cb8\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at Helsinki-NLP/opus-mt-fr-en.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence :\n",
      "You see, in this world there's two kinds of people, my friend: those with loaded guns, and those who dig. You dig.\n",
      "\n",
      "Translated sentence :\n",
      "Vous voyez, dans ce monde il y a deux sortes de gens, mon ami : ceux qui ont des armes charges, et ceux qui creusent. Vous creusez.\n",
      "\n",
      "Translated Back sentence :\n",
      "You see, in this world there are two kinds of people, my friend: those who have loaded weapons, and those who dig. You're digging.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# An example\n",
    "ori = \"You see, in this world there's two kinds of people, my friend: those with loaded guns, and those who dig. You dig.\"\n",
    "model = EasyNMT('opus-mt', max_loaded_models=2)\n",
    "trans = model.translate(ori, source_lang='en', target_lang='fr')\n",
    "back = model.translate(trans, source_lang='fr', target_lang='en')\n",
    "print('Original sentence :\\n', ori, '\\n', sep='')\n",
    "print('Translated sentence :\\n', trans, '\\n', sep='')\n",
    "print('Translated Back sentence :\\n', back, '\\n', sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation of the file 'augmented.csv' \n",
    "Due to CUDA out of memory, the file needs to be generated incrementally, hence the intermediate savings. <br>\n",
    "At each augmentation of a class, it's good to restart the kernel and load the 'augmented.csv' file and <br>\n",
    "continue to perform the augmentation for the next class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "631 2201 4160\n"
     ]
    }
   ],
   "source": [
    "ds = process_labels_csv('memotion_dataset_7k/labels.csv')\n",
    "ds = ds.reset_index()\n",
    "ds = ds.rename(columns={'index': 'meme_id'})\n",
    "ds0 = ds[ds.overall_sentiment == 0]\n",
    "ds1 = ds[ds.overall_sentiment == 1]\n",
    "ds2 = ds[ds.overall_sentiment == 2]\n",
    "model = EasyNMT('opus-mt')\n",
    "n0, n1, n2 = np.unique(ds.overall_sentiment, return_counts=1)[1]\n",
    "print(n0, n1, n2)\n",
    "sentences_0 = list(ds0.text_corrected)\n",
    "sentences_1 = list(ds1.text_corrected)\n",
    "sentences_2 = list(ds2.text_corrected)\n",
    "lang_0 = ['de', 'ru', 'is', 'fr', 'it', 'et', 'es', 'ga', 'hi', 'phi', 'hu', 'uk', 'da']\n",
    "lang_1 = ['de', 'ru', 'is']\n",
    "lang_2 = ['de']\n",
    "model = EasyNMT('opus-mt', max_loaded_models=2*len(lang_0))\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de ru is fr it et es ga hi phi hu uk da "
     ]
    }
   ],
   "source": [
    "for lang in lang_0:\n",
    "    print(lang, end=' ')\n",
    "    for s in range(len(sentences_0) // batch_size + 1):\n",
    "        new_ds0 = ds0.copy().iloc[s*batch_size:(s+1)*batch_size]\n",
    "        translations = model.translate(sentences_0[s*batch_size:(s+1)*batch_size], source_lang='en', target_lang=lang)\n",
    "        new_ds0.text_corrected = model.translate(translations, source_lang=lang, target_lang='en')\n",
    "        ds = pd.concat((ds, new_ds0))\n",
    "        ds.to_csv('augmented.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = pd.read_csv('augmented.csv').drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de ru is "
     ]
    }
   ],
   "source": [
    "for lang in lang_1:\n",
    "    print(lang, end=' ')\n",
    "    for s in range(len(sentences_1) // batch_size + 1):\n",
    "        new_ds1 = ds1.copy().iloc[s*batch_size:(s+1)*batch_size]\n",
    "        translations = model.translate(sentences_1[s*batch_size:(s+1)*batch_size], source_lang='en', target_lang=lang)\n",
    "        new_ds1.text_corrected = model.translate(translations, source_lang=lang, target_lang='en')\n",
    "        ds = pd.concat((ds, new_ds1))\n",
    "        ds.to_csv('augmented.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = pd.read_csv('augmented.csv').drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de "
     ]
    }
   ],
   "source": [
    "for lang in lang_2:\n",
    "    print(lang, end=' ')\n",
    "    for s in range(len(sentences_2) // batch_size + 1):\n",
    "        new_ds2 = ds2.copy().iloc[s*batch_size:(s+1)*batch_size]\n",
    "        translations = model.translate(sentences_2[s*batch_size:(s+1)*batch_size], source_lang='en', target_lang=lang)\n",
    "        new_ds2.text_corrected = model.translate(translations, source_lang=lang, target_lang='en')\n",
    "        ds = pd.concat((ds, new_ds2))\n",
    "        ds.to_csv('augmented.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.reset_index(drop=True)\n",
    "ds.to_csv('augmented.csv')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Project-1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
